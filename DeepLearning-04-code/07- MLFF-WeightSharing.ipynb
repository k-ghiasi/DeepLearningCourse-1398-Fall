{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> <div align=\"center\">In the name of God </div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Author: Sayed Kamaledin Ghiasi-Shrirazi</font> <a href=\"http://profsite.um.ac.ir/~k.ghiasi\">(http://profsite.um.ac.ir/~k.ghiasi)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A multi-layer feedforward neural network with Weight Sharing in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing general modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing PyTorch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the MLFF network by inheriting from nn.Module"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the following command is crucial:\n",
    "\n",
    "``\n",
    "self.layers = nn.ModuleList(layersList)\n",
    "``"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLFF(nn.Module):\n",
    "    def __init__(self, num_hidden_layers, num_input_neurons, \n",
    "                 num_hidden_neurons, num_output_neurons):\n",
    "        super().__init__()\n",
    "        self.layersCount =  num_hidden_layers+1\n",
    "        layersList = [None] * (self.layersCount)\n",
    "        layersList[0] = nn.Linear(num_input_neurons, num_hidden_neurons)\n",
    "        for i in range (1, num_hidden_layers):\n",
    "            layersList[i] = nn.Linear(num_hidden_neurons, num_hidden_neurons)\n",
    "        layersList[num_hidden_layers] = nn.Linear(num_hidden_neurons, num_output_neurons)\n",
    "        self.layers  = nn.ModuleList(layersList)\n",
    "        # Share all hidden layer weights\n",
    "        for i in range (2, num_hidden_layers):\n",
    "            layersList[i].weight = layersList[1].weight\n",
    "            layersList[i].bias = layersList[1].bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i in range(self.layersCount):\n",
    "            x = self.layers[i](x)\n",
    "            if (i < self.layersCount - 1):\n",
    "                x = F.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLFF(\n",
      "  (layers): ModuleList(\n",
      "    (0): Linear(in_features=784, out_features=100, bias=True)\n",
      "    (1): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (2): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (3): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (4): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (5): Linear(in_features=100, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = MLFF(5, 784, 100, 10)\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MnistTrainX = sio.loadmat ('../../datasets/mnist/MnistTrainX')['MnistTrainX'] / 255;\n",
    "MnistTrainY = sio.loadmat ('../../datasets/mnist/MnistTrainY')['MnistTrainY'];\n",
    "MnistTestX  = sio.loadmat ('../../datasets/mnist/MnistTestX')['MnistTestX'] / 255;\n",
    "MnistTestY  = sio.loadmat ('../../datasets/mnist/MnistTestY')['MnistTestY'];\n",
    "\n",
    "N = 60000\n",
    "MnistTrainX = MnistTrainX[:N,:]\n",
    "MnistTrainY = MnistTrainY[:N,:]\n",
    "XTrain = MnistTrainX\n",
    "yTrain = MnistTrainY.squeeze()\n",
    "XTest = MnistTestX\n",
    "yTest = MnistTestY.squeeze()\n",
    "N, dim = XTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 2\n",
    "batch_size = 100\n",
    "report_after_X_iterations = 100\n",
    "learning_rate = 0.1\n",
    "num_batches = N // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- iteration #0 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 2.303187847137451, Accuracy on training data = 10.218333333333334%\n",
      "\n",
      "---- iteration #100 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 2.299666404724121, Accuracy on training data = 11.236666666666666%\n",
      "\n",
      "---- iteration #200 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 2.294888973236084, Accuracy on training data = 11.236666666666666%\n",
      "\n",
      "---- iteration #300 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 2.2778592109680176, Accuracy on training data = 16.181666666666665%\n",
      "\n",
      "---- iteration #400 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 1.1601697206497192, Accuracy on training data = 54.56166666666667%\n",
      "\n",
      "---- iteration #500 of 600 at epoch #0 of 2 ---- :\n",
      "Loss = 0.6362378001213074, Accuracy on training data = 78.55166666666666%\n",
      "\n",
      "---- iteration #0 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.4121897518634796, Accuracy on training data = 86.30833333333334%\n",
      "\n",
      "---- iteration #100 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.3385835587978363, Accuracy on training data = 91.42666666666666%\n",
      "\n",
      "---- iteration #200 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.6405994892120361, Accuracy on training data = 85.25333333333333%\n",
      "\n",
      "---- iteration #300 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.21999405324459076, Accuracy on training data = 93.57833333333333%\n",
      "\n",
      "---- iteration #400 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.19746990501880646, Accuracy on training data = 94.94166666666666%\n",
      "\n",
      "---- iteration #500 of 600 at epoch #1 of 2 ---- :\n",
      "Loss = 0.22740256786346436, Accuracy on training data = 94.675%\n"
     ]
    }
   ],
   "source": [
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr= learning_rate)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    for itr in range (num_batches):\n",
    "        X = torch.tensor (MnistTrainX[itr*batch_size:(itr+1)*batch_size,:], dtype=torch.float)\n",
    "        T = MnistTrainY[itr*batch_size:(itr+1)*batch_size]\n",
    "        T = torch.tensor (T.squeeze(), dtype = torch.long)\n",
    "        X = X.to(device)\n",
    "        T = T.to(device)\n",
    "        output = net(X)\n",
    "        loss = criterion(output, T)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (itr % report_after_X_iterations == 0):\n",
    "            print('\\n---- iteration #{0} of {1} at epoch #{2} of {3} ---- :'.format(\n",
    "                itr, num_batches, epoch, num_epochs))\n",
    "            score = 0.0\n",
    "            for i in range(num_batches):\n",
    "                X = MnistTrainX[i * batch_size:(i + 1) * batch_size, :]\n",
    "                T = MnistTrainY[i * batch_size:(i + 1) * batch_size]\n",
    "                T = T.squeeze()\n",
    "                X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "                #T = torch.tensor(T, dtype=torch.long).to(device)\n",
    "                output = net(X)\n",
    "                prediction = torch.argmax(output, dim=1).cpu().numpy()\n",
    "                score += np.sum(prediction == T)\n",
    "            score /= N\n",
    "            score *= 100\n",
    "            print('Loss = {0}, Accuracy on training data = {1}%'.format(loss.item(), score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
