{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color='green'> <div align=\"center\">In the name of God </div></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color='red'> Author: Sayed Kamaledin Ghiasi-Shrirazi</font> <a href=\"http://profsite.um.ac.ir/~k.ghiasi\">(http://profsite.um.ac.ir/~k.ghiasi)</a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN on MNIST with PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing general modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing PyTorch modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining network by inheriting from nn.Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/BVLC/caffe/blob/master/examples/mnist/lenet.prototxt\n",
    "class SemiLeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=20, kernel_size=5,padding =0)\n",
    "        self.conv2 = nn.Conv2d(in_channels=20, out_channels=40, kernel_size=5,padding =0)\n",
    "        self.lin1  = nn.Linear(in_features=40*4*4, out_features=500)\n",
    "        self.lin2  = nn.Linear(in_features=500, out_features=10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.max_pool2d(F.relu (x), kernel_size=2, stride = 2)      \n",
    "        x = self.conv2(x)\n",
    "        x = F.max_pool2d(F.relu (x), kernel_size=2, stride = 2)\n",
    "        x = x.view(-1, 40*4*4)\n",
    "        x = self.lin1(x)\n",
    "        x = F.relu (x)\n",
    "        x = self.lin2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SemiLeNet(\n",
      "  (conv1): Conv2d(1, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(20, 40, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (lin1): Linear(in_features=640, out_features=500, bias=True)\n",
      "  (lin2): Linear(in_features=500, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = SemiLeNet()\n",
    "print (net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print (device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "MnistTrainX = sio.loadmat ('../../datasets/mnist/MnistTrainX')['MnistTrainX'] / 255;\n",
    "MnistTrainY = sio.loadmat ('../../datasets/mnist/MnistTrainY')['MnistTrainY'];\n",
    "MnistTestX  = sio.loadmat ('../../datasets/mnist/MnistTestX')['MnistTestX'] / 255;\n",
    "MnistTestY  = sio.loadmat ('../../datasets/mnist/MnistTestY')['MnistTestY'];\n",
    "\n",
    "N = 60000\n",
    "MnistTrainX = MnistTrainX[:N,:]\n",
    "MnistTrainY = MnistTrainY[:N,:]\n",
    "XTrain = MnistTrainX\n",
    "yTrain = MnistTrainY.squeeze()\n",
    "XTest = MnistTestX\n",
    "yTest = MnistTestY.squeeze()\n",
    "N, dim = XTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "batch_size = 100\n",
    "report_after_X_iterations = 600\n",
    "learning_rate = 0.01\n",
    "num_batches = N // batch_size\n",
    "NTest = 10000\n",
    "num_test_batches = NTest // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---- iteration #600 of 600 at epoch #1 of 20 ---- :\n",
      "Loss = 0.2428543120622635, Accuracy on training data = 96.93833333333333%\n",
      "Loss = 0.2428543120622635, Accuracy on testing data = 97.32%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #2 of 20 ---- :\n",
      "Loss = 0.23620660603046417, Accuracy on training data = 98.10166666666666%\n",
      "Loss = 0.23620660603046417, Accuracy on testing data = 98.02%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #3 of 20 ---- :\n",
      "Loss = 0.22546741366386414, Accuracy on training data = 98.40833333333333%\n",
      "Loss = 0.22546741366386414, Accuracy on testing data = 98.13%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #4 of 20 ---- :\n",
      "Loss = 0.21167248487472534, Accuracy on training data = 98.72%\n",
      "Loss = 0.21167248487472534, Accuracy on testing data = 98.35000000000001%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #5 of 20 ---- :\n",
      "Loss = 0.2015366554260254, Accuracy on training data = 99.02499999999999%\n",
      "Loss = 0.2015366554260254, Accuracy on testing data = 98.57000000000001%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #6 of 20 ---- :\n",
      "Loss = 0.19735831022262573, Accuracy on training data = 99.19%\n",
      "Loss = 0.19735831022262573, Accuracy on testing data = 98.69%\n",
      "Learning rate after scheduler.step(): 0.01\n",
      "\n",
      "---- iteration #600 of 600 at epoch #7 of 20 ---- :\n",
      "Loss = 0.19146481156349182, Accuracy on training data = 99.355%\n",
      "Loss = 0.19146481156349182, Accuracy on testing data = 98.81%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #8 of 20 ---- :\n",
      "Loss = 0.16989243030548096, Accuracy on training data = 99.53999999999999%\n",
      "Loss = 0.16989243030548096, Accuracy on testing data = 99.05000000000001%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #9 of 20 ---- :\n",
      "Loss = 0.16908596456050873, Accuracy on training data = 99.58500000000001%\n",
      "Loss = 0.16908596456050873, Accuracy on testing data = 99.03999999999999%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #10 of 20 ---- :\n",
      "Loss = 0.16631624102592468, Accuracy on training data = 99.63333333333333%\n",
      "Loss = 0.16631624102592468, Accuracy on testing data = 99.00999999999999%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #11 of 20 ---- :\n",
      "Loss = 0.1631055772304535, Accuracy on training data = 99.66166666666668%\n",
      "Loss = 0.1631055772304535, Accuracy on testing data = 99.02%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #12 of 20 ---- :\n",
      "Loss = 0.1604442447423935, Accuracy on training data = 99.695%\n",
      "Loss = 0.1604442447423935, Accuracy on testing data = 99.02%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #13 of 20 ---- :\n",
      "Loss = 0.15696562826633453, Accuracy on training data = 99.71833333333333%\n",
      "Loss = 0.15696562826633453, Accuracy on testing data = 99.03%\n",
      "Learning rate after scheduler.step(): 0.003\n",
      "\n",
      "---- iteration #600 of 600 at epoch #14 of 20 ---- :\n",
      "Loss = 0.15288960933685303, Accuracy on training data = 99.73833333333333%\n",
      "Loss = 0.15288960933685303, Accuracy on testing data = 99.06%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #15 of 20 ---- :\n",
      "Loss = 0.14603109657764435, Accuracy on training data = 99.84666666666666%\n",
      "Loss = 0.14603109657764435, Accuracy on testing data = 99.15%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #16 of 20 ---- :\n",
      "Loss = 0.14338891208171844, Accuracy on training data = 99.85000000000001%\n",
      "Loss = 0.14338891208171844, Accuracy on testing data = 99.18%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #17 of 20 ---- :\n",
      "Loss = 0.14100679755210876, Accuracy on training data = 99.855%\n",
      "Loss = 0.14100679755210876, Accuracy on testing data = 99.18%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #18 of 20 ---- :\n",
      "Loss = 0.13891862332820892, Accuracy on training data = 99.86833333333334%\n",
      "Loss = 0.13891862332820892, Accuracy on testing data = 99.16%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #19 of 20 ---- :\n",
      "Loss = 0.13716773688793182, Accuracy on training data = 99.87166666666667%\n",
      "Loss = 0.13716773688793182, Accuracy on testing data = 99.16%\n",
      "Learning rate after scheduler.step(): 0.0009\n",
      "\n",
      "---- iteration #600 of 600 at epoch #20 of 20 ---- :\n",
      "Loss = 0.13504339754581451, Accuracy on training data = 99.87666666666667%\n",
      "Loss = 0.13504339754581451, Accuracy on testing data = 99.16%\n"
     ]
    }
   ],
   "source": [
    "net.to(device)\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr= learning_rate, momentum=0.9)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, \n",
    "                                           milestones=[7,14], gamma=0.3)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range (num_epochs):\n",
    "    if epoch:\n",
    "        scheduler.step()\n",
    "        new_lr = optimizer.param_groups[0]['lr']\n",
    "        print (F'Learning rate after scheduler.step(): {new_lr}')\n",
    "    for itr in range (num_batches):\n",
    "        X = torch.tensor (MnistTrainX[itr*batch_size:(itr+1)*batch_size,:], dtype=torch.float)\n",
    "        X = X.view (-1,1,28,28)\n",
    "        T = MnistTrainY[itr*batch_size:(itr+1)*batch_size]\n",
    "        T = torch.tensor (T.squeeze(), dtype = torch.long)\n",
    "        X = X.to(device)\n",
    "        T = T.to(device)\n",
    "        output = net(X)\n",
    "        loss = criterion(output, T)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if ((itr+1) % report_after_X_iterations == 0):\n",
    "            print('\\n---- iteration #{0} of {1} at epoch #{2} of {3} ---- :'.format(\n",
    "                itr+1, num_batches, epoch+1, num_epochs))\n",
    "            score = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_batches):\n",
    "                    X = MnistTrainX[i * batch_size:(i + 1) * batch_size, :]\n",
    "                    X = np.reshape (X, (-1,1,28,28))\n",
    "                    T = MnistTrainY[i * batch_size:(i + 1) * batch_size]\n",
    "                    T = T.squeeze()\n",
    "                    X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "                    #T = torch.tensor(T, dtype=torch.long).to(device)\n",
    "                    output = net(X)\n",
    "                    prediction = torch.argmax(output, dim=1).cpu().numpy()\n",
    "                    score += np.sum(prediction == T)\n",
    "            score /= N\n",
    "            score *= 100\n",
    "            print('Loss = {0}, Accuracy on training data = {1}%'.format(loss.item(), score))\n",
    "            \n",
    "            \n",
    "            score = 0.0\n",
    "            with torch.no_grad():\n",
    "                for i in range(num_test_batches):\n",
    "                    X = MnistTestX[i * batch_size:(i + 1) * batch_size, :]\n",
    "                    X = np.reshape (X, (-1,1,28,28))\n",
    "                    T = MnistTestY[i * batch_size:(i + 1) * batch_size]\n",
    "                    T = T.squeeze()\n",
    "                    X = torch.tensor(X, dtype=torch.float).to(device)\n",
    "                    output = net(X)\n",
    "                    prediction = torch.argmax(output, dim=1).cpu().numpy()\n",
    "                    score += np.sum(prediction == T)\n",
    "            score /= NTest\n",
    "            score *= 100\n",
    "            print('Loss = {0}, Accuracy on testing data = {1}%'.format(loss.item(), score))            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
